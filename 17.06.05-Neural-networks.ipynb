{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In my research, I have been using existing libraries to build neural network models. For example, [Keras](https://keras.io/) provides an extremely modular and clear Python API which sits on top of [Tensorflow](https://www.tensorflow.org/). It is very easy to train and experiment with neural networks, without needing to understand all the details under the hood. This was creating using the content of [Deep Learning Book](http://www.deeplearningbook.org/) chapter 6.\n",
    "\n",
    "Sometimes it's good to understand all of the details. I would like to better understand the backpropagation algorithm. As such, I am implementing a Multi Layer Perception from scratch using only Python and Numpy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural networks can be thought of as sequence of tranformations made to input data.\n",
    "\n",
    "$$y = f(x)$$\n",
    "\n",
    "Deep neural networks make very many such transformation, which creates complex representations of the data.\n",
    "\n",
    "$$y = f(f...f(f(x)))$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given examples of known $x$ and $y$, we want to define a mapping between them which reliably takes as input $x$ values and outputs values in $y$. Further, we want to capture some underlying structure that relates $x$ and $y$, meaning if another example $x^*$ are sampled in the same way as $x$, and the model has not seen it during training, we can predict ahead of time what the corresponding $y^*$ would be."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What form does this tranformation $f$ take?\n",
    "\n",
    "We take:\n",
    "\n",
    "$$ f = g(0, w^Tx + c) $$\n",
    "\n",
    "where $w$ and $c$ are vectors. $w^Tx + c$ constitutes what is known as an [affine map](https://en.wikipedia.org/wiki/Affine_transformation). Affine maps are transformations that preserve points, lines and planes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "g(z) = $max(0,z)$ is a known as the [Relu](https://en.wikipedia.org/wiki/Rectifier_(neural_networks) operator. This simple non-linear operation allows the series of functions $y = f(f...f(f(x)))$ to become non-linear. Note that if each $f$ only comprised the affine map $w^Tx + c$, then the resulting composition $y = f(f...f(f(x)))$ would always be linear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note this this is similar to the formulation of logistic regression. In logistic regression we find vectors $w$ and $c$ such that:\n",
    "\n",
    "$$y = \\sigma(w^T x + c)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "minimises the cross-entropy of the true and predicted distributions. Minimising this cross entropy defines the cost function. Cross entropy is defined as:\n",
    "\n",
    "$$H(p, q) = - \\sum_i p_i(x) \\log q_i(x) = \\mathop{{}\\mathbb{E}}[-\\log q] = H(p) + D_{KL}(p\\|q) $$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $H(p) = -\\sum_i p_i\\log p_i$ is the [entropy](https://en.wikipedia.org/wiki/Entropy_(information_theory) of $p$ and $D_{KL}(p\\|q) = \\sum_i p_i\\log \\frac{p_{i}}{q_{i}}$ is the [Kullback-Leibler divergence](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence) of $p$ given $q$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case of our real values $y$ and our predicted values $y'$. Since we are predicting probabilities, $p \\in \\{y, 1-y\\}$ and $q \\in \\{y', 1-y'\\}$ the cross-entropy loss function is defined by:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$L(y,y') = - \\sum_i p_i(x) \\log q_i(x) = \\sum_i \\bigl(y_i \\log y_i' + (1-y_i) \\log (1-y_i')\\bigr)$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Chain Rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In calculus, the chain rule is used to define the derivate of composite functions. If $y = g(x)$ and $z = f(g(x)) = f(y)$, then the chain rule states that:\n",
    "\n",
    "$$ \\frac{dz}{dx} = \\frac{df(g(x))}{dx} = \\frac{df(y)}{dx} =  \\frac{df}{dy}\\frac{dy}{dx} = \\frac{dz}{dy}\\frac{dy}{dx} $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This generalizes beyond the scalar case. Suppose that $x \\in \\mathbb{R}^m$, $y \\in \\mathbb{R}^n$. Let $g$ be a mapping from $\\mathbb{R}^m$ to $\\mathbb{R}^n$, and let $f$ map from $\\mathbb{R}^n$ to $\\mathbb{R}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then,\n",
    "\n",
    "$$ \\frac{\\partial z}{\\partial x_i} = \\sum_{j}\\frac{\\partial z}{\\partial y_j}\\frac{\\partial y_j}{\\partial x_i}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In vector notation this is represented by:\n",
    "\n",
    "$$\\nabla_\\mathbf{x} z = \\Bigl( \\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{x}} \\Bigr)^T \\nabla_{\\mathbf{x}} z$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $\\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{x}}$ is the $n \\times m$ Jacobian matrix of $g$. We read this as the gradient of $z$ with respect to the vector $\\mathbf{x}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see from this that we obtain the gradient of a variable $\\mathbf{x}$ by multiplying a Jacobian matrix $\\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{x}}$ by a gradient $\\Delta_{\\mathbf{x}} z$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the context of deep learning we do not just operate on vectors, but also on tensors. We can compute the gradient of $z$ with respect to a tensor $X$ as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\nabla_\\mathbf{x} z = \\sum_j (\\nabla_{\\mathbf{X}} Y_j) \\frac{\\partial z}{\\partial Y_j}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normally, $(\\nabla_{\\mathbf{X}} z)_i$ corresponds to $\\frac{\\partial z}{\\partial X_i}$. For a vector, this enumerates all possible elements in the vector. For tensors, we just flatten across all the axis to achieve one long vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now describe the propagation algorithm:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Algorithm**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$l$ Network depth. <br>\n",
    "$\\mathbf{W}^{(i)}, i \\in \\{1,...l\\}$ weight parameters <br>\n",
    "$\\mathbf{b}^{(i)}, i \\in \\{1,...l\\}$ bias parameters <br>\n",
    "$\\mathbf{x}$ input\n",
    "$\\mathbf{y}$ target output <br>\n",
    "\n",
    "* $\\mathbf{h}^{(0)}$ = $x$\n",
    "* **for** $k = 1, ..., l$ **do**\n",
    "    * $\\mathbf{a}^{(k)} = \\mathbf{b}^{(k)} + \\mathbf{W}^{(k)}h^{(k)}$\n",
    "    * $\\mathbf{h}^{(k)} = f(\\mathbf{a}^{(k)})$\n",
    "* **end for**\n",
    "* $\\mathbf{y'} = \\mathbf{h}^{(l)}$\n",
    "* J = $L(y',y) + \\lambda \\Omega(\\theta)$\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will implement a Multi Layer Perception with a single hidden layer. As such, $\\mathbf{W}$ and $\\mathbf{b}$ have two columns. The number of rows is defined by the number of hidden units, $h$ of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the breast cancer data dataset. This defines a set of 30 predicters that are used to classify malignant vs benign breast cancer tumours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((569, 30), (569,))\n"
     ]
    }
   ],
   "source": [
    "import scipy\n",
    "import sklearn.datasets\n",
    "cancer_data = sklearn.datasets.load_breast_cancer()\n",
    "y = cancer_data.target\n",
    "X = cancer_data.data\n",
    "X = (X - np.mean(X,axis=0)) / np.std(X,axis=0)\n",
    "print (X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We randomly initialise the weight and bias parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def rand_init(s,h):\n",
    "    return np.random.random(size=(s,h)) - 0.5\n",
    "W = {1: rand_init(30,h), 2: rand_init(5,1)}\n",
    "h = {}\n",
    "b = {1: rand_init(1,1), 2: rand_init(1,1)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compute the forward pass where the network transforms the input $X$ input output $y$. We convert the final output of the neural network to a binary prediction using the $\\sigma$ function.\n",
    "\n",
    "$$ \\sigma(x) = \\frac{1}{1 + \\exp(-x)} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def forward_pass(X):\n",
    "\n",
    "    a = {}\n",
    "    h[0] = X\n",
    "    for k in [1,2]:\n",
    "        a[k] = b[k] + np.dot(h[k-1],W[k])\n",
    "        h[k] = a[k].copy()\n",
    "        h[k][h[k] < 0] = 0\n",
    "    y_pred = h[2]\n",
    "    y_pred = sigmoid(y_pred)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = forward_pass(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backward pass "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the backward pass, we need to define the cost function, $J$, as the cross-entropy between $y$ and $y'$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def cost(y_real,y_pred):\n",
    "    return -np.sum(y_real*np.log(y_pred) + (1-y_real)*np.log(1-y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The derivate of the $\\sigma$ function is:\n",
    "\n",
    "$$ \\frac{d \\sigma}{dx} = \\sigma(x)(1 - \\sigma(x))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to compute the backward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dsigmoid(x):\n",
    "    return sigmoid(x)*(1 - sigmoid(x))\n",
    "\n",
    "\n",
    "\n",
    "def backward_pass(y_pred):\n",
    "    grad_y_pred = (y_pred - y) * dsigmoid(y_pred) # the last layer's error\n",
    "    grad_w2 = h[1].T.dot(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.dot(W[2].T) # the second laye's error \n",
    "    grad_h = grad_h_relu.copy()\n",
    "    grad_h[h < 0] = 0  # the derivate of ReLU\n",
    "    grad_w1 = h[0].T.dot(grad_h)\n",
    "\n",
    "\n",
    "    W[1] -= learning_rate * grad_w1\n",
    "    W[2] -= learning_rate * grad_w2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fonz/anaconda/envs/deep/lib/python2.7/site-packages/ipykernel/__main__.py:11: VisibleDeprecationWarning: using a boolean instead of an integer will result in an error in the future\n"
     ]
    }
   ],
   "source": [
    "backward_pass(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.005\n",
    "W = {1: rand_init(30,5), 2: rand_init(5,1)}\n",
    "h = {}\n",
    "b = {1: rand_init(1,1), 2: rand_init(1,1)}\n",
    "def train():\n",
    "    for i in range(100):\n",
    "        y_pred = forward_pass(X)\n",
    "        loss = cost(y,y_pred)\n",
    "        if i%20 == 0:\n",
    "            print (loss)\n",
    "        backward_pass(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "456.719292981\n",
      "197.024215512\n",
      "190.130295586\n",
      "189.869291004\n",
      "189.725359574\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fonz/anaconda/envs/deep/lib/python2.7/site-packages/ipykernel/__main__.py:11: VisibleDeprecationWarning: using a boolean instead of an integer will result in an error in the future\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:deep]",
   "language": "python",
   "name": "conda-env-deep-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": false,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
